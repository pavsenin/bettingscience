{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-5954071f2c1a>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Avsenin.Pavel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Avsenin.Pavel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Avsenin.Pavel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Avsenin.Pavel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Avsenin.Pavel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen(data, batch_n):\n",
    "    inds = range(data.shape[0])\n",
    "    np.random.shuffle(inds)\n",
    "    for i in range(data.shape[0] / batch_n):\n",
    "        ii = inds[i*batch_n:(i+1)*batch_n]\n",
    "        yield data[ii, :]\n",
    "def he_initializer(size):\n",
    "    return tf.random_normal_initializer(mean=0.0, stddev=np.sqrt(1./size), seed=None, dtype=tf.float32)\n",
    "def linear_layer(tensor, input_size, out_size, init_fn=he_initializer):\n",
    "    W = tf.get_variable('W', shape=[input_size, out_size], initializer=init_fn(input_size))\n",
    "    b = tf.get_variable('b', shape=[out_size], initializer=tf.constant_initializer(0.1))\n",
    "    return tf.add(tf.matmul(tensor, W), b)\n",
    "def sample_prior(loc=0., scale=1., size=(64, 10)):\n",
    "    return np.tanh(np.random.normal(loc=loc, scale=scale, size=size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAE(object):\n",
    "    def __init__(self, batch_size=64, input_space=28*28, latent_space=10, p=3., middle_layers=None,\n",
    "                activation=tf.nn.tanh, learning_rate=0.001, l2_lambda=0.001, initializer=he_initializer):\n",
    "        self.batch_size = batch_size\n",
    "        self.input_space = input_space\n",
    "        self.latent_space = latent_space\n",
    "        self.p = p\n",
    "        self.middle_layers = [1024, 1024]\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initializer = initializer\n",
    "        tf.reset_default_graph()\n",
    "        self.input_x = tf.placeholder(tf.float32, [None, input_space])\n",
    "        self.z_tensor = tf.placeholder(tf.float32, [None, latent_space])\n",
    "        \n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            self._encoder()\n",
    "        self.encoded = self.encoder_layers[-1]\n",
    "        \n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            self.decoder_layers = self._decoder(self.encoded)\n",
    "            self.decoded = self.decoder_layers[-1]\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            self.generator_layers = self._decoder(self.z_tensor)\n",
    "            self.generated = tf.nn.sigmoid(self.generator_layers[-1], name=\"generated\")\n",
    "        sizes = [64, 64, 1]\n",
    "        with tf.variable_scope(\"discriminator\"):\n",
    "            self.disc_layers_neg = self._discriminator(self.encoded, sizes)\n",
    "            self.disc_neg = self.disc_layers_neg[-1]\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            self.disc_layers_pos = self._discriminator(self.z_tensor, sizes)\n",
    "            self.disc_pos = self.disc_layers_pos[-1]\n",
    "            \n",
    "        self.pos_loss = tf.nn.relu(self.disc_pos) - self.disc_pos + tf.log(1.0 + tf.exp(-tf.abs(self.disc_pos)))\n",
    "        self.neg_loss = tf.nn.relu(self.disc_neg) - self.disc_neg + tf.log(1.0 + tf.exp(-tf.abs(self.disc_neg)))\n",
    "        self.disc_loss = tf.reduce_mean(tf.add(self.pos_loss, self.neg_loss))\n",
    "        self.enc_loss = tf.reduce_mean(tf.subtract(self.neg_loss, self.disc_neg))\n",
    "        batch_logloss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.decoded, labels=self.input_x), 1)\n",
    "        self.ae_loss = tf.reduce_mean(batch_logloss)\n",
    "        disc_ws = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='discriminator')\n",
    "        ae_ws = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='encoder') + tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='decoder')\n",
    "        self.l2_loss = tf.multiply(tf.reduce_sum([tf.nn.l2_loss(ws) for ws in ae_ws]), l2_lambda)\n",
    "        self.gen_loss = tf.add(tf.add(self.enc_loss, self.ae_loss), self.l2_loss)\n",
    "        \n",
    "        with tf.variable_scope('optimizers'):\n",
    "            self.train_discriminator = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.disc_loss, var_list=disc_ws)\n",
    "            self.train_generator = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.gen_loss, var_list=ae_ws)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def _encoder(self):\n",
    "        sizes = [self.input_space] + self.middle_layers + [self.latent_space]\n",
    "        self.encoder_layers = [self.input_x]\n",
    "        for i in range(len(sizes) - 1):\n",
    "            with tf.variable_scope('layer-%s' % i):\n",
    "                linear = linear_layer(self.encoder_layers[-1], sizes[i], sizes[i+1])\n",
    "                self.encoder_layers.append(self.activation(linear))\n",
    "                \n",
    "    def _decoder(self, tensor):\n",
    "        sizes = [self.latent_space] + self.middle_layers[::-1]\n",
    "        decoder_layers = [tensor]\n",
    "        for i in range(len(sizes) - 1):\n",
    "            with tf.variable_scope('layer-%s' % i):\n",
    "                linear = linear_layer(decoder_layers[-1], sizes[i], sizes[i+1])\n",
    "                decoder_layers.append(self.activation(linear))\n",
    "        with tf.variable_scope('output-layer'):\n",
    "            linear = linear_layer(decoder_layers[-1], sizes[-1], self.input_space)\n",
    "            decoder_layers.append(linear)\n",
    "        return decoder_layers\n",
    "    \n",
    "    def _discriminator(self, tensor, sizes):\n",
    "        sizes = [self.latent_space] + sizes + [1]\n",
    "        disc_layers = [tensor]\n",
    "        for i in range(len(sizes) - 1):\n",
    "            with tf.variable_scope('layer-%s' % i):\n",
    "                linear = linear_layer(disc_layers[-1], sizes[i], sizes[i+1])\n",
    "                disc_layers.append(self.activation(linear))\n",
    "        with tf.variable_scope('class-layer'):\n",
    "            linear = linear_layer(disc_layers[-1], sizes[-1], self.input_space)\n",
    "            disc_layers.append(linear)\n",
    "        return disc_layers\n",
    "    \n",
    "    def train(self):\n",
    "        sess = self.sess\n",
    "        test_x = mnist.test.images\n",
    "        gloss = 0.69\n",
    "        \n",
    "        for i in range(1000):\n",
    "            batch_x, _ = mnist.train.next_batch(self.batch_size)\n",
    "            if gloss > np.log(self.p):\n",
    "                gloss, _ = sess.run([self.enc_loss, self.train_generator], feed_dict={self.input_x: batch_x})\n",
    "            else:\n",
    "                batch_z = sample_prior(scale=1.0, size=(len(batch_x), self.latent_space))\n",
    "                gloss, _ = sess.run([self.enc_loss, self.train_discriminator], feed_dict={self.input_x:batch_x, self.z_tensor:batch_z})\n",
    "            if i % 1000 == 0:\n",
    "                gtd = aae.sess.run(aae.generated, feed_dict={aae.z_tensor:sample_prior(size=(4, 10))})\n",
    "                pixels = gtd[0, :].reshape(28, 28)\n",
    "                plt.imshow(pixels, cmap='gray')\n",
    "                plt.show()\n",
    "                \n",
    "                #plot_mnist(gtd.reshape([4, 28, 28]), [1, 4])\n",
    "                #pixels = gtd.reshape([4, 28, 28])\n",
    "                #fig=plt.figure(figsize=(8, 8))\n",
    "                #for i in range(0, 4):\n",
    "                #    fig.add_subplot(1, 4, i+1)\n",
    "                #    plt.imshow(pixels[i], cmap='gray')\n",
    "                #plt.show()\n",
    "aae = AAE()\n",
    "for i in range(200):\n",
    "    aae.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
