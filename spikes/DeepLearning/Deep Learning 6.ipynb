{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "input_fname = 'onegin.txt'\n",
    "output_fname = 'onegin_output.txt'\n",
    "model_fname = 'onegin_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_CHAR = '\\b'\n",
    "END_CHAR = '\\t'\n",
    "PADDING_CHAR = '\\a'\n",
    "chars = set([START_CHAR, '\\n', END_CHAR])\n",
    "with open(input_fname, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        chars.update(list(line.strip().lower()))\n",
    "char_indices = { c : i for i, c in enumerate(sorted(list(chars))) }\n",
    "char_indices[PADDING_CHAR] = 0\n",
    "indices_to_chars = { i : c for c, i in char_indices.items() }\n",
    "num_chars = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one(i, sz):\n",
    "    res = np.zeros(sz)\n",
    "    res[i] = 1\n",
    "    return res\n",
    "char_vectors = {\n",
    "    c : (np.zeros(num_chars) if c == PADDING_CHAR else get_one(v, num_chars))\n",
    "    for c,v in char_indices.items()\n",
    "}\n",
    "sentence_end_markers = set('.!?')\n",
    "sentences = []\n",
    "current_sentence = ''\n",
    "with open(input_fname, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        s = line.strip().lower()\n",
    "        if len(s) > 0:\n",
    "            current_sentence += s + '\\n'\n",
    "        if len(s) == 0 or s[-1] in sentence_end_markers:\n",
    "            current_sentence = current_sentence.strip()\n",
    "            if len(current_sentence) > 10 and not current_sentence.startswith('. . . . . . . .'):\n",
    "                sentences.append(current_sentence)\n",
    "            current_sentence = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(sentences):\n",
    "    max_sentence_len = np.max([len(x) for x in sentences])\n",
    "    X = np.zeros((len(sentences), max_sentence_len, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), max_sentence_len, len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        char_seq = (START_CHAR + sentence + END_CHAR).ljust(max_sentence_len + 1, PADDING_CHAR)\n",
    "        for t in range(max_sentence_len):\n",
    "            X[i, t, :] = char_vectors[char_seq[t]]\n",
    "            y[i, t, :] = char_vectors[char_seq[t + 1]]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avsenin.Pavel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "C:\\Users\\Avsenin.Pavel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"tanh\", return_sequences=True, input_shape=(None, 84), units=128)`\n",
      "  \"\"\"\n",
      "C:\\Users\\Avsenin.Pavel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=84)`\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, TimeDistributed, Activation\n",
    "from keras.optimizers import Adam\n",
    "model = Sequential()\n",
    "model.add(LSTM(output_dim=128, activation='tanh', return_sequences=True, input_dim=num_chars))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(output_dim=num_chars)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(clipnorm=1.), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = np.random.choice(range(len(sentences)), int(len(sentences) * 0.05))\n",
    "sentences_train = [sentences[x] for x in set(range(len(sentences)))- set(test_indices)]\n",
    "sentences_test = [sentences[x] for x in test_indices]\n",
    "sentences_train = sorted(sentences_train, key = lambda x: len(x))\n",
    "X_test, y_test = get_matrices(sentences_test)\n",
    "batch_size = 16\n",
    "def generate_batch():\n",
    "    while True:\n",
    "        for i in range(int(len(sentences_train) / batch_size)):\n",
    "            sentences_batch = sentences_train[i*batch_size : (i+1)*batch_size]\n",
    "            yield get_matrices(sentences_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, CSVLogger, Callback\n",
    "class CharSampler(Callback):\n",
    "    def __init__(self, char_vectors, model):\n",
    "        self.char_vectors = char_vectors\n",
    "        self.model = model\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.epoch = 0\n",
    "        if os.path.isfile(output_fname):\n",
    "            os.remove(output_fname)\n",
    "    def sample(self, preds, temperature=1.0):\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = np.log(preds) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        probas = np.random.multinomial(1, preds, 1)\n",
    "        return np.argmax(probas)\n",
    "    def sample_one(self, T):\n",
    "        result = START_CHAR\n",
    "        while len(result) < 500:\n",
    "            Xsampled = np.zeros((1, len(result), num_chars))\n",
    "            for t, c in enumerate(list(result)):\n",
    "                Xsampled[0,t,:] = self.char_vectors[c]\n",
    "            ysampled = self.model.predict(Xsampled, batch_size=1)[0,:]\n",
    "            yv = ysampled[len(result)-1,:]\n",
    "            selected_char = indices_to_chars[self.sample(yv, T)]\n",
    "            if selected_char == END_CHAR:\n",
    "                break\n",
    "            result = result + selected_char\n",
    "        return result\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.epoch = self.epoch+1\n",
    "        if self.epoch % 1 == 0:\n",
    "            print(\"\\nEpoch %d text sampling:\" % self.epoch)\n",
    "            with open(output_fname, 'a', encoding='utf-8') as outf:\n",
    "                outf.write('\\n==== Epoch %d ====\\n' % self.epoch)\n",
    "                for T in [0.3, 0.5, 0.7, 0.9, 1.1]:\n",
    "                    print('tsampling, T = %.1f...' % T)\n",
    "                    for _ in range(5):\n",
    "                        self.model.reset_states()\n",
    "                        res = self.sample_one(T)\n",
    "                        outf.write('\\nT = %.1f\\n%s\\n' % (T, res[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_sampler = CharSampler(char_vectors, model)\n",
    "cb_logger = CSVLogger('sin_l/' + model_fname + '.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1168/1168 [==============================] - 127s 109ms/step - loss: 0.0395 - acc: 0.0490 - val_loss: 0.0138 - val_acc: 0.0010\n",
      "\n",
      "Epoch 1 text sampling:\n",
      "tsampling, T = 0.3...\n",
      "tsampling, T = 0.5...\n",
      "tsampling, T = 0.7...\n",
      "tsampling, T = 0.9...\n",
      "tsampling, T = 1.1...\n",
      "Epoch 2/3\n",
      "1168/1168 [==============================] - 120s 103ms/step - loss: 0.0351 - acc: 0.0503 - val_loss: 0.0149 - val_acc: 0.0011\n",
      "\n",
      "Epoch 2 text sampling:\n",
      "tsampling, T = 0.3...\n",
      "tsampling, T = 0.5...\n",
      "tsampling, T = 0.7...\n",
      "tsampling, T = 0.9...\n",
      "tsampling, T = 1.1...\n",
      "Epoch 3/3\n",
      "1168/1168 [==============================] - 122s 105ms/step - loss: 0.0317 - acc: 0.0512 - val_loss: 0.0152 - val_acc: 9.1383e-04\n",
      "\n",
      "Epoch 3 text sampling:\n",
      "tsampling, T = 0.3...\n",
      "tsampling, T = 0.5...\n",
      "tsampling, T = 0.7...\n",
      "tsampling, T = 0.9...\n",
      "tsampling, T = 1.1...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x9a905fe4e0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generate_batch(),\n",
    "    int(len(sentences_train) / batch_size) * batch_size,\n",
    "    epochs=3, verbose=True, validation_data = (X_test, y_test),\n",
    "    callbacks=[cb_sampler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avsenin.Pavel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"tanh\", return_sequences=True, units=128)`\n",
      "  \"\"\"\n",
      "C:\\Users\\Avsenin.Pavel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"tanh\", return_sequences=True, units=128)`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Avsenin.Pavel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"tanh\", return_sequences=True, units=128)`\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Avsenin.Pavel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=84)`\n",
      "C:\\Users\\Avsenin.Pavel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import merge, Input\n",
    "from keras.models import Model\n",
    "\n",
    "vec = Input(shape=(None, num_chars))\n",
    "l1 = LSTM(output_dim=128, activation='tanh', return_sequences=True)(vec)\n",
    "l1_d = Dropout(0.2)(l1)\n",
    "\n",
    "input2 = merge.Concatenate()([vec, l1_d])\n",
    "l2 = LSTM(output_dim=128, activation='tanh', return_sequences=True)(input2)\n",
    "l2_d = Dropout(0.2)(l2)\n",
    "\n",
    "input3 = merge.Concatenate()([vec, l2_d])\n",
    "l3 = LSTM(output_dim=128, activation='tanh', return_sequences=True)(input3)\n",
    "l3_d = Dropout(0.2)(l3)\n",
    "\n",
    "input_d = merge.Concatenate()([l1_d, l2_d, l3_d])\n",
    "dense3 = TimeDistributed(Dense(output_dim=num_chars))(input_d)\n",
    "output_res = Activation('softmax')(dense3)\n",
    "model = Model(input=vec, output=output_res)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(clipnorm=1.), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1168/1168 [==============================] - 306s 262ms/step - loss: 0.1637 - acc: 0.0155 - val_loss: 0.0098 - val_acc: 0.0012\n",
      "\n",
      "Epoch 1 text sampling:\n",
      "tsampling, T = 0.3...\n",
      "tsampling, T = 0.5...\n",
      "tsampling, T = 0.7...\n",
      "tsampling, T = 0.9...\n",
      "tsampling, T = 1.1...\n",
      "Epoch 2/3\n",
      " 426/1168 [=========>....................] - ETA: 3:20 - loss: 0.1255 - acc: 0.0234"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generate_batch(),\n",
    "    int(len(sentences_train) / batch_size) * batch_size,\n",
    "    epochs=3, verbose=True, validation_data = (X_test, y_test),\n",
    "    callbacks=[cb_sampler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
